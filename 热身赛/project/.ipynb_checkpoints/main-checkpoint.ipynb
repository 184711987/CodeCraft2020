{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_file = \"./data/train_data.txt\"\n",
    "test_file = \"./data/test_data.txt\"\n",
    "answer_file = \"./data/answer.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 1000)\n",
      "(80000,)\n",
      "(20000, 1000)\n",
      "(20000, 1)\n",
      "CPU times: user 23.8 s, sys: 3.05 s, total: 26.8 s\n",
      "Wall time: 42.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_train = pd.read_table(train_file, header=None, encoding='gb2312', sep=',')\n",
    "df_test = pd.read_table(test_file, header=None, encoding='gb2312', sep=',')\n",
    "df_answer = pd.read_table(answer_file, header=None, encoding='gb2312', sep=',')\n",
    "df_train.rename(columns={1000:'label'}, inplace=True)\n",
    "df_label = df_train['label']\n",
    "df_label.shape\n",
    "del df_train['label']\n",
    "df_train.shape\n",
    "print(df_train.shape)\n",
    "print(df_label.shape)\n",
    "print(df_test.shape)\n",
    "print(df_answer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正确:  16860\n",
      "错误:  3140\n",
      "准确率:  0.843\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=5000,\n",
      "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=1e-07, verbose=0,\n",
      "                   warm_start=False)\n",
      "CPU times: user 30 s, sys: 1.26 s, total: 31.3 s\n",
      "Wall time: 22.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(max_iter=5000, tol=0.0000001, multi_class=\"ovr\")\n",
    "lr.fit(df_train, df_label)\n",
    "\n",
    "predictY = lr.predict(df_test)\n",
    "ac, wa = 0, 0\n",
    "for py, y in zip(predictY, df_answer[0]):\n",
    "    if py == y:\n",
    "        ac += 1\n",
    "    else:\n",
    "        wa += 1\n",
    "print(\"正确: \", ac)\n",
    "print(\"错误: \", wa)\n",
    "print(\"准确率: \", ac / (ac + wa))\n",
    "print(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "# date: 2017-12-28\n",
    "# author: emerson\n",
    "from autograd import grad\n",
    "import autograd.numpy as np\n",
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "EPOCHS = 1000\n",
    "\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, loss, weights, lr=0.001, beta1=0.9, beta2=0.999, epislon=1e-8):\n",
    "        self.loss = loss\n",
    "        self.theta = weights\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epislon = epislon\n",
    "        self.get_gradient = grad(loss)\n",
    "        self.m = 0\n",
    "        self.v = 0\n",
    "        self.t = 0\n",
    "\n",
    "    def minimize_raw(self):\n",
    "        self.t += 1\n",
    "        g = self.get_gradient(self.theta)\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * g\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (g * g)\n",
    "        self.m_cat = self.m / (1 - self.beta1 ** self.t)\n",
    "        self.v_cat = self.v / (1 - self.beta2 ** self.t)\n",
    "        self.theta -= self.lr * self.m_cat / (self.v_cat ** 0.5 + self.epislon)\n",
    "\n",
    "    def minimize(self):\n",
    "        self.t += 1\n",
    "        g = self.get_gradient(self.theta)\n",
    "        lr = self.lr * (1 - self.beta2 ** self.t) ** 0.5 / \\\n",
    "            (1 - self.beta1 ** self.t)\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * g\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * (g * g)\n",
    "        self.theta -= lr * self.m / (self.v ** 0.5 + self.epislon)\n",
    "\n",
    "    def minimize_show(self, epochs=5000):\n",
    "        for _ in range(epochs):\n",
    "            self.t += 1\n",
    "            g = self.get_gradient(self.theta)\n",
    "            lr = self.lr * (1 - self.beta2 ** self.t) ** 0.5 / \\\n",
    "                (1 - self.beta1 ** self.t)\n",
    "            self.m = self.beta1 * self.m + (1 - self.beta1) * g\n",
    "            self.v = self.beta2 * self.v + (1 - self.beta2) * (g * g)\n",
    "            self.theta -= lr * self.m / (self.v ** 0.5 + self.epislon)\n",
    "            print(\"step{: 4d} g:{} lr:{} m:{} v:{} theta:{}\".format(\n",
    "                self.t, g, lr, self.m, self.v, self.theta))\n",
    "\n",
    "        final_loss = self.loss(self.theta)\n",
    "    # print(\"final loss:{} weights:{}\".format(final_loss, self.theta))\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 0.5*(np.tanh(x) + 1)\n",
    "\n",
    "\n",
    "def logistic_predictions(weights, inputs):\n",
    "    # Outputs probability of a label being true according to logistic model.\n",
    "    return sigmoid(np.dot(inputs, weights))\n",
    "\n",
    "\n",
    "def training_loss(weights):\n",
    "    # Training loss is the negative log-likelihood of the training labels.\n",
    "    preds = logistic_predictions(weights, inputs)\n",
    "    label_probabilities = preds * targets + (1 - preds) * (1 - targets)\n",
    "    return -np.sum(np.log(label_probabilities))\n",
    "\n",
    "\n",
    "# Build a toy dataset.\n",
    "inputs = np.array([[0.52, 1.12,  0.77],\n",
    "                   [0.88, -1.08, 0.15],\n",
    "                   [0.52, 0.06, -1.30],\n",
    "                   [0.74, -2.49, 1.39]])\n",
    "targets = np.array([True, True, False, True])\n",
    "weights = np.array([0.0, 0.0, 0.0])\n",
    "\n",
    "\n",
    "def sgd(epochs=1000):\n",
    "    training_gradient_fun = grad(training_loss)\n",
    "    # Optimize weights using gradient descent.\n",
    "    weights = np.array([0.0, 0.0, 0.0])\n",
    "    print(\"Initial loss:{}\".format(training_loss(weights)))\n",
    "    for i in range(epochs):\n",
    "        weights -= training_gradient_fun(weights) * 0.01\n",
    "\n",
    "    print(\"Trained loss:{}\".format(training_loss(weights)))\n",
    "    print(\"weights:{}\".format(weights))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    adam = Adam(training_loss, weights, lr=0.01)\n",
    "    print(\"start to optimize:\")\n",
    "    # adam.minimize_show(epochs=EPOCHS)\n",
    "\n",
    "    import pandas as pd\n",
    "    train_df = pd.read_table(\"./data/train_data.txt\")\n",
    "    train_test = pd.read_table(\"./data/test_data.txt\")\n",
    "\n",
    "    targets = train_df[['1000']]\n",
    "\n",
    "\n",
    "    for i in range(EPOCHS):\n",
    "        adam.minimize_raw()\n",
    "    print(\"weights:{} loss:{}\".format(adam.theta, adam.loss(adam.theta)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.  1.5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([34.5, 92.5])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "f  = lambda x, y: (1.5 - x + x*y)**2 + (2.25 - x + x*y**2)**2 + (2.625 - x + x*y**3)**2\n",
    "\n",
    "def loss(weights):\n",
    "    x,y = weights\n",
    "    return f(x,y)\n",
    "\n",
    "weights = np.array([1., 1.5])\n",
    "get_gradient = grad(loss)\n",
    "\n",
    "print(weights)\n",
    "get_gradient(weights)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6.5 64-bit",
   "language": "python",
   "name": "python36564bitdd8a1b9c90524d09a57c469dc70ef4fe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
